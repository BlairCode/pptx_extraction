Title: N/A
Author: huawei
Subject: N/A
Keywords: N/A
Comments: generated using python-pptx
Last Modified By: 厚冰 张
Created: 2013-01-27 09:14:16
Modified: 2025-02-11 12:31:16
Category: N/A
Content Status: N/A
Identifier: N/A
Language: N/A
Revision: 5

现在我们来看看第 1 张幻灯片：
那么我们说说 深度学习基础。
值得一提的是 人工智能学院核心课程。
那么我们说说 2024年春季学期。
这里有个重点 本课程将深入讲解深度学习的基本原理、核心算法及典型应用，涵盖神经网络、卷积神经网络、生成对抗网络等内容，帮助学生掌握深度学习的前沿技术。。

那我们就先讲到这里，接下来看看新的内容。

这一节我们讲讲第 2 张幻灯片：
那么我们说说 课程内容概览。
那么我们说说 1. 神经网络基本原理。
这里有个重点 神经元模型/激活函数/前向传播。
那么我们说说 2. 卷积神经网络。
这里有个重点 局部感受野/池化操作/经典架构。
那么我们说说 3. 生成对抗网络。
首先是 博弈论框架/生成器判别器。
这里有个重点 4. 注意力机制。
值得一提的是 Self-Attention/Transformer架构。
这里有个重点 5. 强化学习基础。
值得一提的是 马尔可夫决策过程/Q-Learning。

那我们就先讲到这里，接下来看看新的内容。

现在我们来看看第 3 张幻灯片：
这里有个重点 神经网络的基本概念。
首先是 神经网络的三大要素：。
比如说：
 加权求和，挺关键的吧？
这些要点就先讲到这里。
简单解释一下：z = ∑w_i x_i + b，它描述了模型的计算过程。
这里包括：
 w_i: 权重，挺关键的吧？
接着是 x_i: 输入，挺关键的吧？
然后是 b: 偏置，挺关键的吧？
另外一点 非线性激活，挺关键的吧？
这些要点就先讲到这里。
简单解释一下：σ(z) = max(0,z) (ReLU)，它让模型更有效。
这里包括：
 引入非线性，挺关键的吧？
还有呢 解决梯度消失问题，挺关键的吧？
然后是 损失函数，挺关键的吧？
这些要点就先讲到这里。
简单解释一下：L = ½(y - ŷ)^2 (均方误差)，它让模型更有效。
这里包括：
 y: 真实值，挺关键的吧？
另外一点 ŷ: 预测值，挺关键的吧？

那我们就先讲到这里，接下来看看新的内容。

接下来聊聊第 4 张幻灯片：
那么我们说说 卷积运算可视化。
这里有个重点 卷积运算的核心思想：。
先来看看：
 卷积核（Kernel）：一个小的权重矩阵，用于提取局部特征。，挺关键的吧？
然后是 步长（Stride）：卷积核在输入上滑动的步幅，影响输出特征图的大小。，挺关键的吧？
然后是 填充（Padding）：在输入边缘添加额外的像素，控制输出特征图的尺寸。，挺关键的吧？
然后是 特征图（Feature Map）：卷积运算的输出，反映了输入中某种特征的分布。，挺关键的吧？

那我们就先讲到这里，接下来看看新的内容。

让我们进入第 5 张幻灯片：
那么我们说说 典型应用场景。
这里有个重点 医疗影像分析。
那么我们说说 CT图像分割。
先来看看：
 肿瘤检测，挺关键的吧？
接着是 器官定位，挺关键的吧？
这些要点就先讲到这里。
这里有个重点 深度学习在医疗影像中的应用显著提高了诊断的准确性和效率。。
那么我们说说 自动驾驶。
那么我们说说 实时目标检测。
比如说：
 行人识别，挺关键的吧？
另外一点 车道线检测，挺关键的吧？
这些要点就先讲到这里。
这里有个重点 自动驾驶技术依赖于深度学习模型对复杂环境。
首先是 的实时感知和决策。。
这里有个重点 艺术创作。
首先是 风格迁移示例。
我们聊聊：
 图像风格化，挺关键的吧？
另外一点 视频风格化，挺关键的吧？
这些要点就先讲到这里。
首先是 生成对抗网络（GAN）为艺术创作提供了全新的可能性。。

那我们就先讲到这里，接下来看看新的内容。

让我们进入第 6 张幻灯片：
这里有个重点 知识点详解。
值得一提的是 1. 神经网络的基本结构：。
这里包括：
 神经网络由输入层、隐藏层和输出层组成。，挺关键的吧？
另外一点 每一层包含多个神经元，神经元之间通过权重连接。，挺关键的吧？
然后是 输入层接收原始数据，隐藏层提取特征，输出层生成最终结果。，挺关键的吧？
这些要点就先讲到这里。
值得一提的是 2. 激活函数的作用：。
这里包括：
 激活函数引入非线性，使神经网络能够学习复杂的模式。，挺关键的吧？
还有呢 常用的激活函数包括ReLU、Sigmoid和Tanh。，挺关键的吧？
接着是 ReLU（Rectified Linear Unit）是目前最常用的激活函数，因其简单且有效。，挺关键的吧？
这里有张图，具体内容可以参考幻灯片。
这有个图片说明，具体内容可以参考幻灯片。
我们看个示例，具体内容可以参考幻灯片。
这里有张图，具体内容可以参考幻灯片。
这些要点就先讲到这里。

好了，今天的内容就到这儿，希望大家收获满满，下次再聊！